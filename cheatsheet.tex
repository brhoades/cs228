\documentclass[fontsize=5pt]{scrartcl}

%
% Original Page by LinuxMercedes
%

\usepackage[
        nohead,
        nofoot,
        left=0.55in,
        right=0.55in,
        top=0.55in,
        bottom=0.55in,
]{geometry}

\usepackage{amsmath,scalefnt}

\renewcommand*{\arraystretch}{.5}

\usepackage{multicol}
\setlength{\columnsep}{5pt}

\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}

\pagenumbering{gobble}

\usepackage{enumitem}
\setlist[itemize]{itemsep=-2pt, itemindent=0pt, leftmargin=*}
\setlist[enumerate]{itemsep=-2pt, itemindent=0pt, leftmargin=*}

\usepackage[compact]{titlesec}
\titlespacing{\section}{-1pt}{-1pt}{-1pt}
\titlespacing{\subsection}{-1pt}{-1pt}{-1pt}

\usepackage{listings}

%Y hoy yo reza que no empleo ni alma pobre encontrar√° esta magica negra.
%This is a custom 'tight' matrix for this cheatsheet. It's ugly.
\newenvironment{tmatrix}%
{ 
  \scalefont{.5}
  \setlength{\tabcolsep}{5pt}
  $\left[\hspace{-3.5pt}\begin{array}{c@{\hspace{1pt}}@{\hspace{1pt}}c@{\hspace{1pt}}c@{\hspace{1pt}}|@\hspace{0pt}c}
}%
{
   \end{array}\hspace{-3.5pt}\right]$
}

\newenvironment{tmatrix3}%
{ 
  %\scalefont{.5}
  %\setlength{\tabcolsep}{5pt}
  $\left[\hspace{-3.5pt}\begin{array}{c@{\hspace{1pt}}@{\hspace{1pt}}c@{\hspace{1pt}}c@{\hspace{3pt}}}
}%
{
   \end{array}\hspace{-3.5pt}\right]$
}

\newenvironment{tmatrix1}%
{ 
  $\left[\hspace{-3.5pt}\begin{array}{c@{\hspace{3pt}}}
}%
{
   \end{array}\hspace{-3.5pt}\right]$
}


%was 3 3 3 3
\DeclareMathSizes{3pt}{3pt}{3pt}{3pt}

\begin{document}

\begin{multicols}{3}
  \section{Definitions}
    \begin{itemize}
      \item \textbf{Analytical Solution}: using a formula to give an exact answer.
      \item \textbf{Numerical Solution}: obtaining an approximation for the answer, 
            ie with a Taylor Series, to get a equation which can be solved with available
            information.
    \end{itemize}

  \section{Problems}
    \begin{enumerate}
      \item \textbf{Gaussian Elimination with Partial Pivoting  \underline{30pts}}
        \begin{itemize}
          \item \textbf{Gauss Elimination}: consists of two phases: forward elimination, back substitution. 
            Utilizes a matrix to do this.
            \item \textbf{Forward elimination}: reduces a matrix like so, \textbf{in this order}: \\
              \begin{tmatrix}
                1 & 3 & 1 & 9 \\
                \mathbf{1} & 1 & -1 & 1 \\
                3 & 11 & 5 & 35 \\
              \end{tmatrix} \hspace{-2pt}$\Rightarrow$\hspace{-4pt}%
              \begin{tmatrix}
              1 & 3 & 1 & 9 \\
              0 & -2 & -2 & -8 \\
              \mathbf{0} & 2 & 2 & 8
              \end{tmatrix} \hspace{-2pt}$\Rightarrow$\hspace{-4pt}%
              \begin{tmatrix}
              1 & 3 & 1 & 9 \\
              0 & -2 & -2 & -8 \\
              0 & \mathbf{0} & 0 & 0
              \end{tmatrix}%
            \vspace{-2pt}\\
            It does this using multiples of other rows, added on to the row you are modifying. 
            The diagonal does not matter in this method, and can be any value. The objective is to make
            the bottom triangle zeros, which turns this into the U portion of the A=L\textbf{U}.\\
          \vspace{-6pt}
        \item \textbf{Back substitution}: takes the lowest segment of the matrix, here $0x+0y+0z=0$,
          solves, and plugs it into the next up portion. Here, $0x-2y-2z=-8$, and then further up again.
        \item \textbf{Pivoting}: Pivoting eases the problem by having the largest leading number pivoted
          to the top of the matrix, meaning no fractions of $\frac{1}{832}$ need to be multiplied from another row,
          as the first row is not modified. In the example above, the row would be swapped with the top. 
          Note that only one, whole row can be swapped with one other.
          Pivoting cannot happen in LU decomposition as it breaks the relation in the matrix. 
      \end{itemize}
          
      \item \textbf{LU Decomposition \underline{28pts}}: LU Decomposition mirrors the actions on the the main matrix
          to another, L, which can then be multiplied by U to get A. LU Decomp. is typically used as it has fewer multiplication 
          and division operations and is thus often faster for computers.
          To summarize the process: Let $A=LU$ where A is the original matrix and substitute into $AX=B$. 
              This yields $LUX=B$... So solve for $X$ to get the answer
             and you get $UX= $ for your first equation, followed by $LY=B$ and $UX=Y$. Here is an example form of an equation translated to $AX=B$: \\
               $x_1+x_2-x_3=4 \\
               x_1-2x_2+3x_3=-6 \\
               2x_1+3x_2+x_3=7 \\$
              \begin{tmatrix3}
                1 & 1  & -1 \\
                1 & -2 & 3  \\
                2 & 3  & 1 
              \end{tmatrix3} %
              \begin{tmatrix1}
                x_1\\
                x_2\\
                x_3
              \end{tmatrix1} $=$%
              \begin{tmatrix1}
                 4 \\
                 -6 \\
                 7 \\
              \end{tmatrix1}

          \begin{enumerate}
            \thickmuskip=0mu
            \item Perform Gaussian Elimination up to the point where you begin back substitution. Take this resultant matrix,
            $U$ (for upper). While performing the Gaussian elimination use the shortcut method for determining L. Start with a partial
            L: \\
            
            %%%%%%%%%%
            $L=$
            \begin{tmatrix3}
              1 & 0 & 0 \\
              ? & 1 & 1 \\ 
              ? & ? & 1 \\
            \end{tmatrix3}%
            \vline\hspace{2pt}%
            $U=$
            \begin{tmatrix3}
              1 & 1  & -1 \\
              1 & -2 & 3  \\
              2 & 3  & 1 \\  
            \end{tmatrix3} \\
%%%%%%%%%%%%%%%%%%%%%%%%
            $L=$ %
            \begin{tmatrix3}
             1 & 0 & 0 \\
             \textbf{1} & 1 & 0 \\ 
             ? & ? & 1 \\
            \end{tmatrix3}%
            \vline\hspace{2pt}%
            $U=$
            \begin{tmatrix3}
              1 & 1  & -1 \\
              \textbf{0} & -3 & 4  \\
              2 & 3  & 1 
            \end{tmatrix3}%
             $-R_1+R_2\rightarrow R_2$ \\
%%%%%%%%%%%%%%%%%%%%%%%%
            $L=$ %
            \begin{tmatrix3}
             1 & 0 & 0 \\
             1 & 1 & 0 \\ 
             \mathbf{2} & ? & 1 \\
            \end{tmatrix3}%
            \vline\hspace{2pt}%
            $U=$
            \begin{tmatrix3}
              1 & 1  & -1 \\
              0 & -3 & 4  \\
              \mathbf{0} & 1  & 3 
            \end{tmatrix3}%
             $-2R_1+R_3\rightarrow R_3$ \\
%%%%%%%%%%%%%%%%%%%%%%%%
            $L=$ %
            \begin{tmatrix3}
             1 & 0 & 0 \\
             1 & 1 & 0 \\ 
             2 & \mathbf{\frac{1}{3}} & 1 \\
            \end{tmatrix3}%
            \vline\hspace{2pt}%
            $U=$
            \begin{tmatrix3}
              1 & 1  & -1 \\
              0 & -3 & 4  \\
              0 & 0  & \mathbf{\frac{13}{3}} 
            \end{tmatrix3}%
             $\frac{1}{3}R_2+R_3\rightarrow R_3$ \\
             
           \item While transforming U, record the opposite of the constant multiplied by, into the spot turning to 0 on L (bolded).
                 Now resetup the $LY=B$ equation and solve for $y_1, y_2, y_3$ using forward substitution. \\
              \begin{tmatrix3}
                1 & 0 & 0 \\
                1 & 1 & 0 \\ 
                2 & \frac{1}{3} & 1 \\
              \end{tmatrix3}%
              \begin{tmatrix1}
                y_1\\
                y_2\\
                y_3
              \end{tmatrix1}%
              $=$%
              \begin{tmatrix1}
                4\\
                -6\\
                7
              \end{tmatrix1}\Rightarrow%
              $y_1=4,y_2=-10, y_3=-\frac{13}{3}$
              
            \item Use these values to setup a new matrix to plug into $UX=Y$ and solve the problem with back substitution.\\
              \begin{tmatrix3}
                1 & 1  & -1 \\
                0 & -3 & 4  \\
                0 & 0  & \mathbf{\frac{13}{3}} 
              \end{tmatrix3}%
              \begin{tmatrix1}
                x_1\\
                x_2\\
                x_3\\
              \end{tmatrix1}%
              $=$%
              \begin{tmatrix1}
                4\\
                 -10\\
                 -\frac{13}{3}\\
              \end{tmatrix1}\\
          \end{enumerate}
          
          \item \textbf{Matrix Inversion (LU Decomp Style)}: Solve using a method after LU Decomp. Use $LD=I$ where I is a 3x1 
          chunk of an identity matrix. D is then plugged into $UX=D$ and the resultant X solution is the 3x1 chunk of your inverse matrix.
          
          \begin{enumerate}
          
          \item Plug your lower matrix with some d variables and set it equal to a chunk of your identity matrix (do this 3x with different
                chunks). Solve for each $d$. \\
            \begin{tmatrix3}
              1 & 0 & 0 \\
              1 & 1 & 0 \\ 
              2 & \frac{1}{3} & 1 \\
            \end{tmatrix3}%
            \begin{tmatrix1}
              d_1 \\
              d_2 \\ 
              d_3 \\
            \end{tmatrix1}$=$%
            \begin{tmatrix1}
              1 \\
              0 \\
              0
            \end{tmatrix1}
          
          \item Next plug those D's into a $UX=D$ equation, the resultant X's are chunks of your inverse matrix. \\
            \begin{tmatrix3}
              1 & 1  & -1 \\
              0 & -3 & 4  \\
              0 & 0  & \frac{13}{3}         
            \end{tmatrix3}%
            \begin{tmatrix1}
              x_1 \\
              x_2 \\ 
              x_3 \\
            \end{tmatrix1}$=$%
            \begin{tmatrix1}
              d_1 \\
              d_2 \\ 
              d_3 \\
            \end{tmatrix1}            
          \end{enumerate}
          
         \item Interpolation: Estimation of a function value at an intermediate point that lies between precise data points.
          \begin{itemize}
           \item Newton (Divided Difference) Interpolating Polynomials: Connecting two data points with a straight line. The $f_#(x)$ indicates a 
                 first-order interpolating polynomial. \\
              $N(x) = \sum_{j=0}^{k} a_{j} n_{j}(x)$ \\
              $a_j(x) = f(x_j)$ \\
              $n_j(x) = \prod_{i=0}^{j-1} (x - x_i)$ \\
              This comes out to be something like: \\
              $f_1(x) = f(x_0) + \frac{f(x_1)-f(x_0)}{x_1-x_0}(x-x_0)$ \\
              This gives our general form: \\
              $f[x_i,x_j] = \frac{f(x_i)-f(x_j)}{x_i-x_j}$ \\
              $f[x_i,x_j,x_k] = \frac{f[x_i,x_j]-f[x_i,x_k]}{x_i-x_k}$       
              
              This is usually plugged into a table like so: \\
              \begin{tabular}{|c|c|c|c|}
                \hline
                $x_i$ & $f(x_i)$ & $f[x_j,x_i]$ & $f[x_k,x_j,x_i]$ \\\hline\hline
                $x_0$ & $f(x_0)$ &              &                  \\\hline
                $x_1$ & $f(x_1)$ & $f[x_1,x_0]$ &                  \\\hline
                $x_2$ & $f(x_2)$ & $f[x_2,x_1]$ & $f[x_2,x_1,x_0]$ \\\hline
                \hline
              \end{tabular} \\
              For example: \\
              \begin{tabular}{|c|c|c|c|}
                \hline
                $x_i$ & $f(x_i)$ & $f[x_j,x_i]$ & $f[x_k,x_j,x_i]$ \\\hline\hline
                $x_0$ & 2        &              &                  \\\hline
                $x_1$ & 14       & 6            &                  \\\hline
                $x_2$ & 74       & 60           & 18               \\\hline
                \hline
              \end{tabular} \\
              So: \\
              $f_1(x) = 2+6*(x-0)$\\
              $f_2(x) = 2+6*(x-0)+18(x-0)(x-2)$\\
              $f_3(x) = 2+6*(x-0)+18(x-0)(x-2)+9(x-0)(x-2)(x-3)$\\
              $f_4(x) = 2+6x+18x(x-2)+9x(x-2)(x-3)+1x(x-2)(x-3)(x-4)=x^4‚Äìx^2+2 $ \\
              
              Error Estimation in $n^{th}$ order polynomial: $R_n=f_{n+1}(x)-f_n(x)\Rightarrow f_{n+1}(x)=f_n(x)+R_n$
              
          \item Lagrange Interpolating Polynomials: The Lagrange interpolating polynomial is simply a reformulation of the 
                Newton‚Äôs polynomial that avoids the computation of divided differences.
                
                \begin{math}
                  f_n(x)=\sum_{n}{i=0}{\ell_i(x)f(x_i)} \\
                  \ell_j(x)=\prod_{\begin{smallmatrix}0\le m\le k\\ m\neq j\end{smallmatrix}}\frac{x-x_m}{x_j-x_m}\\
                  = \frac{(x-x_0)}{(x_j-x_0)} \cdots \frac{(x-x_{j-1})}{(x_j-x_{j-1})} \frac{(x-x_{j+1})}{(x_j-x_{j+1})} \cdots \frac{(x-x_k)}{(x_j-x_k)} \\
                  \\
                  f_1(x)=\frac{x-x_1}{x_0-x_1}f(x_0)+\frac{x-x_0}{x_1-x_0}f(x_1)\\
                  f_2(x)=\frac{(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)}f(x_0) \\+\frac{(x-x_0)(x-x_2)}{(x_1-x_0)(x_1-x_2)}f(x_1)}+\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)}
                \end{math}

          \end{itemize}

    \end{enumerate}
    
  \section{Formulas}
      \begin{itemize}
        \item \textbf{True Error}: $E_t =$ True value - Approximation ($\pm$)
        \item \textbf{True \% Rel. Error}: $\epsilon_t = (\frac{\text{True value} - \text{Approximation}}{\text{True Value}})\cdot100\%$
        \item \textbf{Aprox. Rel. Error}: $\epsilon_a =  |\frac{\text{This Approx.} - \text{Last Approx}}{\text{This Approx.}}|\cdot100\%$
      \end{itemize}
      
  \section{Algorithms}
    
      
  \section{Reference}
    \subsection{Matlab}
      \begin{itemize}
        \item \textbf{Common Functions}:
          sqrt, 
          exp, 
          abs,  
          log, 
          log10, 
          factorial, 
          sin, 
          sind, 
          cos,  
          cosd, 
          tan, 
          tand, 
          cot, 
          cotd, 
          round, 
          fix, 
          floor, 
          rem(x,y)\textit{mod}
          ceil, 
          sprintf
        \item \textbf{Matrices} are defined in many ways, here are the most common:
          \lstset{language=Matlab}
          \begin{lstlisting}
a = [ 1 2 3 4 5 ]
b = [ 1 2; 3 4; 5 6 ]
c = [ 1 2
      3 4
      5 6 ]
          \end{lstlisting}%
          $%
          a = \left( 1 2 3 4 5 \right)\hspace{1mm}
          b = \left( \begin{array}{cc}
                      1 & 2 \\
                      3 & 4 \\ 
                      5 & 6 \\
                    \end{array}
                    \right)
          c = b
          $
        \end{itemize}
    \subsection{Matlab Examples}
      \begin{itemize}
        \item Bisection function in Matlab. Passes f(x), lower x, upper x, and a error / upper i.
          \lstset{language=Matlab,basicstyle=\footnotesize,breaklines=true}
        \begin{lstlisting}
% A sample call:
%bisection2(@func1, -2, 4,
% 0.001, 500)
function root = bisection(fx, xl,
  xu, es, imax);

if fx(xl)*fx(xu) > 0 % if 
% guesses do not bracket
    disp('no bracket')
    return
end

for i=1:1:imax
   xr=(xu+xl)/2
   ea = abs((xu-xl)/xl);
   test= fx(xl)*fx(xr);
   if test < 0
       xu=xr;
   end
   if test > 0 xl=xr; end
   if test == 0 ea=0; end
   if ea < es break; end
end
%root = xr, it# = i, fx is original function
        \end{lstlisting}
        \item Newton's Method example code
          \lstset{language=Matlab,basicstyle=\footnotesize,breaklines=true}
          \begin{lstlisting}
function root = newtraph(func,dfunc,xr,es,maxit)
% Newton-Raphson method to find the root of a function
% call syntax:   newtraph(@func,@dfunc,xguess,es,maxit)
% 'func'(function), and 'dfunc'(its derivative) are defined 
% in files func.m and dfunc.m in the same directory
% inputs:
%   func = name of function 
%   dfunc = name of derivative of function 
%   xr = initial guess
%   es = stopping criterion (%)
%   maxit = (optional) maximum allowable iterations
% output:
%   root = real root

iter = 0;
while (1)
  xrold = xr;
  xr = xr - func(xr)/dfunc(xr);
  iter = iter + 1;
  if xr ~= 0, ea = abs((xr - xrold)/xr) * 100; end
  if ea <= es | iter >= maxit, break, end
end
fprintf('\n  Root= %f    #Iterations = %d \n', xr,iter);
fprintf(' How close is f(root) to zero?  f(root)= %f  \n', func(xr));
          \end{lstlisting}
        \end{itemize}
    \subsection{Math}
      \begin{itemize}
      \item \textbf{Taylor Series}: Provides a means to predict the value of a function at one point in terms of
             the function value and its derivatives at another. The function must be infinitely differentiable
             at f(a). An infinite number of terms yields an exact result for $x=a$.
        \begin{itemize}
          \item $f(x), x=a$: $\sum_{n=0} ^ {\infty} \frac {f^{(n)}(a)}{n!} \, (x-a)^{n} + R_n$
          \item Remainder, h is step size and $\epsilon$ is the value which makes the final
          equation exact for the nth derivative: \\ $R_n = \frac{f^{n+1}(\epsilon)}{(n+1)!} (x_{i+1}-x_i)^{n+1}h_{n+1}$ \\
          
        \end{itemize}
      \item \textbf{Matrix Multiplication}: Matrix multiplication is done by going right on the left matrix while going
        down on the right. After a row on the left is done, move to the next and repeat with the next column. Matrices'
        sizes are denoted by c x r, and for two matrices to multiply together, $r_1 = c_2$. Examples:
        \begin{itemize} %thanks wikipedia
          \item $\mathbf{A} = \begin{pmatrix} 
                a & b
                \end{pmatrix}\,, \quad \mathbf{B} = \begin{pmatrix} 
                x \\
                y 
                \end{pmatrix}\,,
                $
                $
                  \begin{pmatrix} 
                  a & b 
                  \end{pmatrix} \begin{pmatrix} 
                  x \\
                  y \\
                  \end{pmatrix} = ax + by \,,
                $ \\
                $
                  \begin{pmatrix} 
                  x \\
                  y \\
                  \end{pmatrix}\begin{pmatrix} 
                  a & b
                  \end{pmatrix} = \begin{pmatrix} 
                  xa & xb \\
                  ya & yb 
                  \end{pmatrix} \,.
                $

        \end{itemize}
      \end{itemize}
  \end{multicols}
\end{document}

