\documentclass[fontsize=5pt]{scrartcl}

%
% Original Page by LinuxMercedes
%

\usepackage[
        nohead,
        nofoot,
        left=0.55in,
        right=0.55in,
        top=0.55in,
        bottom=0.55in,
]{geometry}

\usepackage{amsmath,scalefnt,graphicx}

\renewcommand*{\arraystretch}{.5}

\usepackage{multicol}
\setlength{\columnsep}{5pt}

\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}

\pagenumbering{gobble}

\usepackage{enumitem}
\setlist[itemize]{itemsep=-2pt, itemindent=0pt, leftmargin=*}
\setlist[enumerate]{itemsep=-2pt, itemindent=0pt, leftmargin=*}

\usepackage[compact]{titlesec}
\titlespacing{\section}{-1pt}{-1pt}{-1pt}
\titlespacing{\subsection}{-1pt}{-1pt}{-1pt}

\usepackage{listings}

\newcommand{\colvec}[2][.8]{%
  \scalebox{#1}{%
    $\begin{bmatrix}#2\end{bmatrix}$%
  }
}

%Y hoy yo reza que no empleo ni alma pobre encontrará esta magica negra.
%This is a custom 'tight' matrix for this cheatsheet. It's ugly.
\newenvironment{tmatrix}%
{ 
  \scalefont{.5}
  \setlength{\tabcolsep}{5pt}
  $\left[\hspace{-3.5pt}\begin{array}{c@{\hspace{1pt}}@{\hspace{1pt}}c@{\hspace{1pt}}c@{\hspace{1pt}}|@\hspace{0pt}c}
}%
{
   \end{array}\hspace{-3.5pt}\right]$
}

\newenvironment{tmatrix3}%
{ 
  %\scalefont{.5}
  %\setlength{\tabcolsep}{5pt}
  $\left[\hspace{-3.5pt}\begin{array}{c@{\hspace{1pt}}@{\hspace{1pt}}c@{\hspace{1pt}}c@{\hspace{3pt}}}
}%
{
   \end{array}\hspace{-3.5pt}\right]$
}

\newenvironment{tmatrix1}%
{ 
  $\left[\hspace{-3.5pt}\begin{array}{c@{\hspace{3pt}}}
}%
{
   \end{array}\hspace{-3.5pt}\right]$
}


%was 3 3 3 3
\DeclareMathSizes{3pt}{3pt}{3pt}{3pt}

\begin{document}

\begin{multicols}{3}
  \section{Definitions}
    \begin{itemize}
      \item \textbf{Analytical Solution}: using a formula to give an exact answer.
      \item \textbf{Numerical Solution}: obtaining an approximation for the answer, 
            ie with a Taylor Series, to get a equation which can be solved with available
            information.
    \end{itemize}

  \section{Problems}
    \begin{enumerate}
      \item \textbf{Gaussian Elimination with Partial Pivoting  \underline{30pts}}
        \begin{itemize}
          \item \textbf{Gauss Elimination}: consists of two phases: forward elimination, back substitution. 
            Utilizes a matrix to do this.
            \item \textbf{Forward elimination}: reduces a matrix like so, \textbf{in this order}: \\
              \begin{tmatrix}
                1 & 3 & 1 & 9 \\
                \mathbf{1} & 1 & -1 & 1 \\
                3 & 11 & 5 & 35 \\
              \end{tmatrix} \hspace{-2pt}$\Rightarrow$\hspace{-4pt}%
              \begin{tmatrix}
              1 & 3 & 1 & 9 \\
              0 & -2 & -2 & -8 \\
              \mathbf{0} & 2 & 2 & 8
              \end{tmatrix} \hspace{-2pt}$\Rightarrow$\hspace{-4pt}%
              \begin{tmatrix}
              1 & 3 & 1 & 9 \\
              0 & -2 & -2 & -8 \\
              0 & \mathbf{0} & 0 & 0
              \end{tmatrix}%
            \vspace{-2pt}\\
            It does this using multiples of other rows, added on to the row you are modifying. 
            The diagonal does not matter in this method, and can be any value. The objective is to make
            the bottom triangle zeros, which turns this into the U portion of the A=L\textbf{U}.\\
          \vspace{-6pt}
        \item \textbf{Back substitution}: takes the lowest segment of the matrix, here $0x+0y+0z=0$,
          solves, and plugs it into the next up portion. Here, $0x-2y-2z=-8$, and then further up again.
        \item \textbf{Pivoting}: Pivoting eases the problem by having the largest leading number pivoted
          to the top of the matrix, meaning no fractions of $\frac{1}{832}$ need to be multiplied from another row,
          as the first row is not modified. In the example above, the row would be swapped with the top. 
          Note that only one, whole row can be swapped with one other.
          Pivoting cannot happen in LU decomposition as it breaks the relation in the matrix. 
      \end{itemize}
          
      \item \textbf{LU Decomposition \underline{28pts}}: LU Decomposition mirrors the actions on the the main matrix
          to another, L, which can then be multiplied by U to get A. LU Decomp. is typically used as it has fewer multiplication 
          and division operations and is thus often faster for computers.
          To summarize the process: Let $A=LU$ where A is the original matrix and substitute into $AX=B$. 
              This yields $LUX=B$... So solve for $X$ to get the answer
             and you get $UX= $ for your first equation, followed by $LY=B$ and $UX=Y$. Here is an example form of an equation translated to $AX=B$: \\
               $x_1+x_2-x_3=4 \\
               x_1-2x_2+3x_3=-6 \\
               2x_1+3x_2+x_3=7 \\$
              \begin{tmatrix3}
                1 & 1  & -1 \\
                1 & -2 & 3  \\
                2 & 3  & 1 
              \end{tmatrix3} %
              \begin{tmatrix1}
                x_1\\
                x_2\\
                x_3
              \end{tmatrix1} $=$%
              \begin{tmatrix1}
                 4 \\
                 -6 \\
                 7 \\
              \end{tmatrix1}

          \begin{enumerate}
            \thickmuskip=0mu
            \item Perform Gaussian Elimination up to the point where you begin back substitution. Take this resultant matrix,
            $U$ (for upper). While performing the Gaussian elimination use the shortcut method for determining L. Start with a partial
            L: \\
            
            %%%%%%%%%%
            $L=$
            \begin{tmatrix3}
              1 & 0 & 0 \\
              ? & 1 & 1 \\ 
              ? & ? & 1 \\
            \end{tmatrix3}%
            \vline\hspace{2pt}%
            $U=$
            \begin{tmatrix3}
              1 & 1  & -1 \\
              1 & -2 & 3  \\
              2 & 3  & 1 \\  
            \end{tmatrix3} \\
%%%%%%%%%%%%%%%%%%%%%%%%
            $L=$ %
            \begin{tmatrix3}
             1 & 0 & 0 \\
             \textbf{1} & 1 & 0 \\ 
             ? & ? & 1 \\
            \end{tmatrix3}%
            \vline\hspace{2pt}%
            $U=$
            \begin{tmatrix3}
              1 & 1  & -1 \\
              \textbf{0} & -3 & 4  \\
              2 & 3  & 1 
            \end{tmatrix3}%
             $-R_1+R_2\rightarrow R_2$ \\
%%%%%%%%%%%%%%%%%%%%%%%%
            $L=$ %
            \begin{tmatrix3}
             1 & 0 & 0 \\
             1 & 1 & 0 \\ 
             \mathbf{2} & ? & 1 \\
            \end{tmatrix3}%
            \vline\hspace{2pt}%
            $U=$
            \begin{tmatrix3}
              1 & 1  & -1 \\
              0 & -3 & 4  \\
              \mathbf{0} & 1  & 3 
            \end{tmatrix3}%
             $-2R_1+R_3\rightarrow R_3$ \\
%%%%%%%%%%%%%%%%%%%%%%%%
            $L=$ %
            \begin{tmatrix3}
             1 & 0 & 0 \\
             1 & 1 & 0 \\ 
             2 & \mathbf{\frac{1}{3}} & 1 \\
            \end{tmatrix3}%
            \vline\hspace{2pt}%
            $U=$
            \begin{tmatrix3}
              1 & 1  & -1 \\
              0 & -3 & 4  \\
              0 & 0  & \mathbf{\frac{13}{3}} 
            \end{tmatrix3}%
             $\frac{1}{3}R_2+R_3\rightarrow R_3$ \\
             
           \item While transforming U, record the opposite of the constant multiplied by, into the spot turning to 0 on L (bolded).
                 Now resetup the $LY=B$ equation and solve for $y_1, y_2, y_3$ using forward substitution. \\
              \begin{tmatrix3}
                1 & 0 & 0 \\
                1 & 1 & 0 \\ 
                2 & \frac{1}{3} & 1 \\
              \end{tmatrix3}%
              \begin{tmatrix1}
                y_1\\
                y_2\\
                y_3
              \end{tmatrix1}%
              $=$%
              \begin{tmatrix1}
                4\\
                -6\\
                7
              \end{tmatrix1}\Rightarrow%
              $y_1=4,y_2=-10, y_3=-\frac{13}{3}$
              
            \item Use these values to setup a new matrix to plug into $UX=Y$ and solve the problem with back substitution.\\
              \begin{tmatrix3}
                1 & 1  & -1 \\
                0 & -3 & 4  \\
                0 & 0  & \mathbf{\frac{13}{3}} 
              \end{tmatrix3}%
              \begin{tmatrix1}
                x_1\\
                x_2\\
                x_3\\
              \end{tmatrix1}%
              $=$%
              \begin{tmatrix1}
                4\\
                 -10\\
                 -\frac{13}{3}\\
              \end{tmatrix1}\\
          \end{enumerate}
          
          \item \textbf{Matrix Inversion (LU Decomp Style)}: Solve using a method after LU Decomp. Use $LD=I$ where I is a 3x1 
          chunk of an identity matrix. D is then plugged into $UX=D$ and the resultant X solution is the 3x1 chunk of your inverse matrix.
          
          \begin{enumerate}
          
          \item Plug your lower matrix with some d variables and set it equal to a chunk of your identity matrix (do this 3x with different
                chunks). Solve for each $d$. \\
            \begin{tmatrix3}
              1 & 0 & 0 \\
              1 & 1 & 0 \\ 
              2 & \frac{1}{3} & 1 \\
            \end{tmatrix3}%
            \begin{tmatrix1}
              d_1 \\
              d_2 \\ 
              d_3 \\
            \end{tmatrix1}$=$%
            \begin{tmatrix1}
              1 \\
              0 \\
              0
            \end{tmatrix1}
          
          \item Next plug those D's into a $UX=D$ equation, the resultant X's are chunks of your inverse matrix. \\
            \begin{tmatrix3}
              1 & 1  & -1 \\
              0 & -3 & 4  \\
              0 & 0  & \frac{13}{3}         
            \end{tmatrix3}%
            \begin{tmatrix1}
              x_1 \\
              x_2 \\ 
              x_3 \\
            \end{tmatrix1}$=$%
            \begin{tmatrix1}
              d_1 \\
              d_2 \\ 
              d_3 \\
            \end{tmatrix1}            
          \end{enumerate}
          
         \item \textbf{Interpolation}: Estimation of a function value at an intermediate point that lies between precise data points.
          \begin{itemize}
           \item \textbf{Newton (Divided Difference) Interpolating Polynomials}: Connecting two data points with a straight line. The $f_#(x)$ indicates a 
                 first-order interpolating polynomial. \\
              $N(x) = \sum_{j=0}^{k} a_{j} n_{j}(x)$ \\
              $a_j(x) = f(x_j)$ \\
              $n_j(x) = \prod_{i=0}^{j-1} (x - x_i)$ \\
              This comes out to be something like: \\
              $f_1(x) = f(x_0) + \frac{f(x_1)-f(x_0)}{x_1-x_0}(x-x_0)$ \\
              This gives our general form: \\
              $f[x_i,x_j] = \frac{f(x_i)-f(x_j)}{x_i-x_j}$ \\
              $f[x_i,x_j,x_k] = \frac{f[x_i,x_j]-f[x_i,x_k]}{x_i-x_k}$       
              
              This is usually plugged into a table like so: \\
              \begin{tabular}{|c|c|c|c|}
                \hline
                $x_i$ & $f(x_i)$ & $f[x_j,x_i]$ & $f[x_k,x_j,x_i]$ \\\hline\hline
                $x_0$ & $f(x_0)$ &              &                  \\\hline
                $x_1$ & $f(x_1)$ & $f[x_1,x_0]$ &                  \\\hline
                $x_2$ & $f(x_2)$ & $f[x_2,x_1]$ & $f[x_2,x_1,x_0]$ \\\hline
                \hline
              \end{tabular} \\
              For example: \\
              \begin{tabular}{|c|c|c|c|}
                \hline
                $x_i$ & $f(x_i)$ & $f[x_j,x_i]$ & $f[x_k,x_j,x_i]$ \\\hline\hline
                $x_0$ & 2        &              &                  \\\hline
                $x_1$ & 14       & 6            &                  \\\hline
                $x_2$ & 74       & 60           & 18               \\\hline
                \hline
              \end{tabular} \\
              So: \\
              $f_1(x) = 2+6*(x-0)$\\
              $f_2(x) = 2+6*(x-0)+18(x-0)(x-2)$\\
              $f_3(x) = 2+6*(x-0)+18(x-0)(x-2)+9(x-0)(x-2)(x-3)$\\
              $f_4(x) = 2+6x+18x(x-2)+9x(x-2)(x-3)+1x(x-2)(x-3)(x-4)=x^4–x^2+2 $ \\
              
              Error Estimation in $n^{th}$ order polynomial: $R_n=f_{n+1}(x)-f_n(x)\Rightarrow f_{n+1}(x)=f_n(x)+R_n$
              
          \item \textbf{Lagrange Interpolating Polynomials}: The Lagrange interpolating polynomial is simply a reformulation of the 
                Newton’s polynomial that avoids the computation of divided differences.
                
                \begin{math}
                  f_n(x)=\sum_{n}{i=0}{\ell_i(x)f(x_i)} \\
                  \ell_j(x)=\prod_{\begin{smallmatrix}0\le m\le k\\ m\neq j\end{smallmatrix}}\frac{x-x_m}{x_j-x_m}\\
                  = \frac{(x-x_0)}{(x_j-x_0)} \cdots \frac{(x-x_{j-1})}{(x_j-x_{j-1})} \frac{(x-x_{j+1})}{(x_j-x_{j+1})} \cdots \frac{(x-x_k)}{(x_j-x_k)} \\
                  \\
                  f_1(x)=\frac{x-x_1}{x_0-x_1}f(x_0)+\frac{x-x_0}{x_1-x_0}f(x_1)\\
                  f_2(x)=\frac{(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)}f(x_0) \\+\frac{(x-x_0)(x-x_2)}{(x_1-x_0)(x_1-x_2)}f(x_1)}+\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)}
                \end{math}
          \end{itemize}
          \item \textbf{Curve fitting}: Fit the best curve to a discrete data set and obtain estimates for other data points
          \begin{itemize}
           \item Statistics:
           \begin{itemize}
            \item \textbf{Standard Deviation}: Common measure for spread of a sample (variance = $sqrt^2$): \\
            $S_y = \sqrt{\frac{\sum(y_i-\bar{y})^2}{n-1}}$ \\
            \item \textbf{Coefficent of Variation}: spread of data \\
            $c.v=\frac{S_y}{\bar{y}}*100\%$
          \end{itemize}
          
          \item \textbf{Linear Regression w/ Least Squares}: Fitting a straight line to a set of paired observations. Least squares regression follows. 
            \begin{itemize}
              \item $y_i=a_0+a_1x_i+\textbf{e}$ (e not included)
              \item $e = y_i-a_0-a_1x_i$
              \item $a_1 = m$, y-int$ = a_0$
              \item $S_r=\sum_{n}{i=1}e_i^2\\
                =\sum^{n}_{i=1}(y_{i,measured}-y_{i,model})^2$\\
                $=\sum^{n}_{i=1}(y_i-a_0-a_1x_i)^2$
              \item $S_t=\sum(y_i-\bar{y})^2$
              \item Goodness of fit, r=1 = good: $r^2=\frac{S_t-S_r}{S_t}$
              \item With this, now: \\
                $a_1=\frac{n\sum x_iy_i-\sum x_i\sum y_i}{n\sum x_{i}^2-(\sum x_i)^2}$\\
                $a_0=\bar{y}-a_1\bar{x}$
            \end{itemize}
          \item \textbf{Least squares with nonlinear data}: Some engineering data is poorly represented by a straight line. A curve (polynomial) may be better suited to fit the data.
                      The least squares method can be extended to fit the data to higher order polynomials. 
            \begin{itemize}
              \item $y=a_0+a_1x+a_2x^2$ for order-2 data. Below a mth order polynomial needs this problem solved.
              \item Still minimizing $S_r = \sum_{i=1}^n(y_i-a_0-a_1x_i-a_2x_i^2)^2$\\
              \colvec{$n            & \sum x_i       & \ldots  & \sum x_i^m     \\%
                      \sum x_i     & \sum x_i^2     & \ldots  & \sumx_i^{m+1}  \\%
                      \vdots       & \vdots         & \ddots  & \vdots         \\%
                      \sum x_i^m   & \sum x_i^{m+1} & \ldots  & \sum x_i^{m+m} $%
                      }%
              \colvec{$a_0\\ a_1\\ \vdots \\ a_m$}%
              $=$
              \colvec{$\sum y_i\\ \sum x_iy_i \\ \vdots \\ \sum x_i^my_i$}
            \end{itemize}
            
           \item \textbf{Saturation-growth equation}: Same as Power... $y=a_3\frac{x}{b_3+x}$ but with 1/x on the table.
           
           \item \textbf{Power equation}: Mostly unmentioned. $y=\alpha_2x^{\beta_2}$. log(alpha) is the intercept, while the slope is beta. Do a table translating x and y into log(x) log(y) then perform
                                          linear regression on the table like normal, this yields logy=#*logx-# ($log(y)=\beta_2log(x)+log(\alpha_2)$
          \end{itemize}
        \item Systems of equations
          \begin{itemize}
           \item Transdiagonal systems: bandwidth of three w/ $O(n^3)$ \\
                 \colvec{   {b_1} & {c_1} & {   } & {   } & { 0 } \\
                              {a_2} & {b_2} & {c_2} & {   } & {   } \\
                              {   } & {a_3} & {b_3} & \ddots & {   } \\
                              {   } & {   } & \ddots & \ddots & {c_{n-1}}\\
                              { 0 } & {   } & {   } & {a_n} & {b_n}\\}%
                 \colvec{ x_1\\x_2\\x_3\\\vdots\\x_n}%
                 $=$
                 \colvec{ r_1\\r_2\\r_3\\\vdots\\r_n} \\
          \begin{lstlisting}
DECOMPOSITION

DO  k = 2, n
    a_k  = a_k / b_k-1
    b_k  = b_k - a_k c_k-1

END DO
\end{lstlisting}\\
            \item transdiagonal systems... do forward / backward substitution.
            \end{itemize}

          
    \end{enumerate}
    
  \section{Formulas}
      \begin{itemize}
        \item \textbf{True Error}: $E_t =$ True value - Approximation ($\pm$)
        \item \textbf{True \% Rel. Error}: $\epsilon_t = (\frac{\text{True value} - \text{Approximation}}{\text{True Value}})\cdot100\%$
        \item \textbf{Aprox. Rel. Error}: $\epsilon_a =  |\frac{\text{This Approx.} - \text{Last Approx}}{\text{This Approx.}}|\cdot100\%$
      \end{itemize}
      
  \section{Algorithms}
    
      
  \section{Reference}
    \subsection{Matlab}
      \begin{itemize}
        \item \textbf{Common Functions}:
          sqrt, 
          exp, 
          abs,  
          log, 
          log10, 
          factorial, 
          sin, 
          sind, 
          cos,  
          cosd, 
          tan, 
          tand, 
          cot, 
          cotd, 
          round, 
          fix, 
          floor, 
          rem(x,y)\textit{mod}
          ceil, 
          sprintf
        \item \textbf{Matrices} are defined in many ways, here are the most common:
          \lstset{language=Matlab}
          \begin{lstlisting}
a = [ 1 2 3 4 5 ]
b = [ 1 2; 3 4; 5 6 ]
c = [ 1 2
      3 4
      5 6 ]
          \end{lstlisting}%
          $%
          a = \left( 1 2 3 4 5 \right)\hspace{1mm}
          b = \left( \begin{array}{cc}
                      1 & 2 \\
                      3 & 4 \\ 
                      5 & 6 \\
                    \end{array}
                    \right)
          c = b
          $
        \end{itemize}
    \subsection{Matlab Examples}
      \begin{itemize}
        \item Bisection function in Matlab. Passes f(x), lower x, upper x, and a error / upper i.
          \lstset{language=Matlab,basicstyle=\footnotesize,breaklines=true}
        \begin{lstlisting}
% A sample call:
%bisection2(@func1, -2, 4,
% 0.001, 500)
function root = bisection(fx, xl,
  xu, es, imax);

if fx(xl)*fx(xu) > 0 % if 
% guesses do not bracket
    disp('no bracket')
    return
end

for i=1:1:imax
   xr=(xu+xl)/2
   ea = abs((xu-xl)/xl);
   test= fx(xl)*fx(xr);
   if test < 0
       xu=xr;
   end
   if test > 0 xl=xr; end
   if test == 0 ea=0; end
   if ea < es break; end
end
%root = xr, it# = i, fx is original function
        \end{lstlisting}
        \item Newton's Method example code
          \lstset{language=Matlab,basicstyle=\footnotesize,breaklines=true}
          \begin{lstlisting}
function root = newtraph(func,dfunc,xr,es,maxit)
% Newton-Raphson method to find the root of a function
% call syntax:   newtraph(@func,@dfunc,xguess,es,maxit)
% 'func'(function), and 'dfunc'(its derivative) are defined 
% in files func.m and dfunc.m in the same directory
% inputs:
%   func = name of function 
%   dfunc = name of derivative of function 
%   xr = initial guess
%   es = stopping criterion (%)
%   maxit = (optional) maximum allowable iterations
% output:
%   root = real root

iter = 0;
while (1)
  xrold = xr;
  xr = xr - func(xr)/dfunc(xr);
  iter = iter + 1;
  if xr ~= 0, ea = abs((xr - xrold)/xr) * 100; end
  if ea <= es | iter >= maxit, break, end
end
fprintf('\n  Root= %f    #Iterations = %d \n', xr,iter);
fprintf(' How close is f(root) to zero?  f(root)= %f  \n', func(xr));
          \end{lstlisting}
        \end{itemize}
    \subsection{Math}
      \begin{itemize}
      \item \textbf{Taylor Series}: Provides a means to predict the value of a function at one point in terms of
             the function value and its derivatives at another. The function must be infinitely differentiable
             at f(a). An infinite number of terms yields an exact result for $x=a$.
        \begin{itemize}
          \item $f(x), x=a$: $\sum_{n=0} ^ {\infty} \frac {f^{(n)}(a)}{n!} \, (x-a)^{n} + R_n$
          \item Remainder, h is step size and $\epsilon$ is the value which makes the final
          equation exact for the nth derivative: \\ $R_n = \frac{f^{n+1}(\epsilon)}{(n+1)!} (x_{i+1}-x_i)^{n+1}h_{n+1}$ \\
          
        \end{itemize}
      \item \textbf{Matrix Multiplication}: Matrix multiplication is done by going right on the left matrix while going
        down on the right. After a row on the left is done, move to the next and repeat with the next column. Matrices'
        sizes are denoted by c x r, and for two matrices to multiply together, $r_1 = c_2$. Examples:
        \begin{itemize} %thanks wikipedia
          \item $\mathbf{A} = \begin{pmatrix} 
                a & b
                \end{pmatrix}\,, \quad \mathbf{B} = \begin{pmatrix} 
                x \\
                y 
                \end{pmatrix}\,,
                $
                $
                  \begin{pmatrix} 
                  a & b 
                  \end{pmatrix} \begin{pmatrix} 
                  x \\
                  y \\
                  \end{pmatrix} = ax + by \,,
                $ \\
                $
                  \begin{pmatrix} 
                  x \\
                  y \\
                  \end{pmatrix}\begin{pmatrix} 
                  a & b
                  \end{pmatrix} = \begin{pmatrix} 
                  xa & xb \\
                  ya & yb 
                  \end{pmatrix} \,.
                $

        \end{itemize}
      \end{itemize}
  \end{multicols}
\end{document}

