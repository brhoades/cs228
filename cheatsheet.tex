\documentclass[fontsize=5pt]{scrartcl}

%
% Original Page by LinuxMercedes
%

\usepackage[
        nohead,
        nofoot,
        left=0.55in,
        right=0.55in,
        top=0.55in,
        bottom=0.55in,
]{geometry}

\usepackage{amsmath,scalefnt,graphicx}

\renewcommand*{\arraystretch}{.5}

\usepackage{multicol}
\setlength{\columnsep}{5pt}

\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}

\pagenumbering{gobble}

\usepackage{enumitem}
\setlist[itemize]{itemsep=-2pt, itemindent=0pt, leftmargin=*}
\setlist[enumerate]{itemsep=-2pt, itemindent=0pt, leftmargin=*}

\usepackage[compact]{titlesec}
\titlespacing{\section}{-1pt}{-1pt}{-1pt}
\titlespacing{\subsection}{-1pt}{-1pt}{-1pt}

\usepackage{listings}

\newcommand{\colvec}[2][.8]{%
  \scalebox{#1}{%
    $\begin{bmatrix}#2\end{bmatrix}$%
  }
}

%Y hoy yo reza que no empleo ni alma pobre encontrará esta magica negra.
%This is a custom 'tight' matrix for this cheatsheet. It's ugly.
\newenvironment{tmatrix}%
{ 
  \scalefont{.5}
  \setlength{\tabcolsep}{5pt}
  $\left[\hspace{-3.5pt}\begin{array}{c@{\hspace{1pt}}@{\hspace{1pt}}c@{\hspace{1pt}}c@{\hspace{1pt}}|@\hspace{0pt}c}
}%
{
   \end{array}\hspace{-3.5pt}\right]$
}

\newenvironment{tmatrix3}%
{ 
  %\scalefont{.5}
  %\setlength{\tabcolsep}{5pt}
  $\left[\hspace{-3.5pt}\begin{array}{c@{\hspace{1pt}}@{\hspace{1pt}}c@{\hspace{1pt}}c@{\hspace{3pt}}}
}%
{
   \end{array}\hspace{-3.5pt}\right]$
}

\newenvironment{tmatrix1}%
{ 
  $\left[\hspace{-3.5pt}\begin{array}{c@{\hspace{3pt}}}
}%
{
   \end{array}\hspace{-3.5pt}\right]$
}


%was 3 3 3 3
\DeclareMathSizes{3pt}{3pt}{3pt}{3pt}

\begin{document}

\begin{multicols}{3}
  \section{Definitions}
    \begin{itemize}
      \item \textbf{Analytical Solution}: using a formula to give an exact answer.
      \item \textbf{Numerical Solution}: obtaining an approximation for the answer, 
            ie with a Taylor Series, to get a equation which can be solved with available
            information.
    \end{itemize}

  \section{Topics}
    \begin{enumerate}
      \item \textbf{Newton-Cotes Integration Formulas}: Newton-Cotes formulas replace a complicated function
        or tabular data with an approximating function which is easier to integrate--- like using Reimann sums 
        to get the area under a line, as n increases estimations improve. \\
        \begin{math}
          f_n(x)=a_0+a_1x+\ldots+a_nx^n
        \end{math}
    
      \item \textbf{The Trapezoidal Rule}: Trapezoidal rule uses a trapezoid over points $a$ to $b$
        to estimate the area under the curve (the integral). $I$ denotes the estimate while $E_t$
        denotes the error where $\epsilon$ lies somewhere between $a$ and $b$.\\
        \begin{math}
          I=(b-a)\frac{f(a)+f(b)}{2}
          E_t=-\frac{1}{12}f''(\epsilon})(b-a)^3
        \end{math}
      
      \item \textbf{Multiple-Application Trapezoidal Rule}: The accuracy of the trapezoidal rule is
        greatly improved by dividing the interval from $a$ to $b$ into $n$ segments and applying the
        method to each. $I$ here represents the final result and is simplified from the true equation. 
        $\bar{f}''$ represents the mean of the second derivative over $b$ to $a$. The trapezoidal rule
        is $O(h^2)$, if the number of segments is doubled, the truncation error will be quartered.\\
        \begin{math}
          h=\frac{b-a}{b} \hspace{2mm} a=x_0 \hspace{2mm} b=x_n \\
          I=\frac{b-a}{2n}\left[f(x_0)+f(x_n)+2\sum^{n-1}_{i=1}f(x_i)\right] \\
          E_a=-\frac{(b-a)^3}{12n^2}\bar{f}''
        \end{math}
      
      \item \textbf{Simpson's Rule}: A more accurate estimate even yet can be obtained if a higher-order polynomial is used
      to connect the points given on a tabulated data set. These formulas are called Simpson's rules. \textbf{Simpson's 1/3 Rule}
      results when a \textbf{2nd order Lagrange} interpolating polynomial is used for f(x) (which is below?). \\
      \begin{math}
        I=\frac{h}{3}\Left[f(x_0)+4f(x_1)+f(x_2)\Right]\hspace{2mm}h=\frac{b-a}{2} \\
        E_t=-\frac{1}{2880}(b-a)^5f^{(4)}(\epsilon)
      \end{math}

      \item \textbf{Multiple Application of Simpson's 1/3 Rule}: Like with trapezoidal the results can be improved
        by breaking the problem up. However, it only works when there are an even number of segments and an odd number
        of points where values are equispaced. This formula uses the same base values as the multitrapizoidal above.
        \textbf{Simpson's 3/8ths Rule} uses 3rd order Lagrange interpolating polynomials to four points and integrates
        (which is below?). \\
        
        \begin{math}
          I=\frac{h}{3}\Right[f(x_0)+4\sum_{i=1,3,5...}^{n-1}f(x_i)+ \\ 2\sum_{j=2,4,6...}^{n-2}f(x_j)+f(x_n)\Left]
          E_t=-\frac{1}{6480}(b-a)^5f^{(4)}(\epsilon)
        \end{math}

      
      \item \textbf{Integration with Unequal Segments}: The only example provided was the trapezoidal rule. \\
        \begin{math}
          I=h_1\frac{f(x_0)+f(x_1)}{2}+h_2\frac{f(x_1)+f(x_2)}{2}+\ldots \\
          +h_n\frac{f(x_{n-1})+f(x_{n})}{2}
        \end{math}

      \item \textbf{Romberg Integration}: Uses successive applications of the trapezoidal rule to get more accurate
        estimates of an integral by weighing each estimate appropriately. Works similarly to a previous problem on 
        HW2. I below represents the true value of the integral, while I(h) is the trapezoidal rule (n segs, step h)
        and E(h) is the truncation error). Index $j$ distinguishes between more ($j>>$) and less ($j<<$) accurate 
        estimates while $k=1$ refers directly to the trapezoidal rule.\\
        \begin{math}
          I=I(h)+E(h) \hspace{2mm} h=\frac{b-a}{n} \hspace{2mm} n=\frac{b-a}{h} \\
         % E = \frac{b-a}{12}h^2f''=O(h^2)
          I_{j,k}=\frac{4^{k-1}*I_{j+1,k-1} - I_{j,k-1}}{4^{k-1}-1} \\
          O(h^{2k}) acc. \\
          E_t=I-I(h) \\
         \end{math} 
         For:\\
         \begin{math}
           f(x) = \frac{1}{x} \\
         \end{math}

         It helps to build a table like so: \\
         \begin{math}
           I^0_1 \\
           I^0_2\hspace{2mm}I^1_2 \\
           I^0_3\hspace{2mm}I^1_3\hspace{2mm}I^2_3 \\
         \end{math}
         Then the entire 0 column, far left, can be computed manually, doubling n in the trapezoidal rule. \\
         \begin{math}
           n=1: I^0_1 = (1+\frac{1}{2})\frac{1}{2} = 0.75 \\
           n=2: I^0_2 = (1+\frac{1}{1.5}+\frac{0.5}(1+\frac{1}{2})=0.7083333\\
           n=4: I^0_3 = 0.69702381 \hspace{2mm}%
           n=8: I^0_4 = 0.69412185 \\
         \end{math}
         Now starting with $I^0_1$ and $I^0_2$: \\
         \begin{math}
           I^1_2=\frac{4I^0_2-I^0_1}{3} = 0.694444 \\
           I^1_3=\frac{4I^0_3-I^0_2}{3}=0.693253;\vspace{2mm} I^2_3=\frac{16I^1_3-I^1_2}{15}=0.693... \\
         \end{math}

        \item \textbf{Forward Finite-divided-difference Formulas}: Inaccurate formulas for getting an estimate
          of a derivative. Both are $O(h)$ and $O(h^2)$ respectively.\\
        \begin{math}
          f'(x_i)=\frac{f(x_{i+1})-f(x_i)}{h}\\
          f'(x_i)=\frac{-f(x_{i+2}+4f(x_{i+1})-3f(x_i)}{2h}\\
        \end{math} \\
        \begin{math}
          f''(x_i)=\frac{f(x_{i+2})-2f(x_{i+1})+f(x_i)}{h^2} \\
          f''(x_i)=-\frac{f(x_{i+3})+4f(x_{i+2})-5f(x_{i+1})+2f(x_i)}{h^2} \\
        \end{math} \\
        
        \item \textbf{Backward Finite-divided-difference Formulas}: Like above with errors. \\
        \begin{math}
          f'(x_i)=\frac{f(x_{i})-f(x_{i-1})}{h}\\
          f'(x_i)=\frac{3f(x_{i+2}-4f(x_{i-1})+f(x_{i-2})}{2h}\\
        \end{math} \\
        \begin{math}
          f''(x_i)=\frac{f(x_i)-2f(x_{i-1})+f(x_{i-2})}{h^2} \\
          f''(x_i)=-\frac{2f(x_i)-5f(x_{i-1})+4f(x_{i-2})-f(x_{i-3})}{h^2}
        \end{math}

        \item \textbf{Centered Finite-divided-difference Formulas}: Last $f(x)$ dropped due to size. \\
        \begin{math}
          f'(x_i)=\frac{f(x_{i+1})-f(x_{i-1})}{2h}\\
          f'(x_i)=\frac{-f(x_{i+2}+8f(x_{i+1})-8f(x_{i-1})+f(x_{i-2})}{12h}\\
        \end{math} \\
        \begin{math}
          \frac{f(x_{i+1})-2f(x_i)+f(x_{i-1})}{h^2} \\
          -\frac{-f(x_{i+2})+16f(x_{i+1})-30f(x_i)+16f(x_{i-1})-f(x_{i-2})}{12h^2}
        \end{math}

        \item \textbf{Richardson Extrapolation}: There are two ways to improve derivative estimates when employing 
          finite divided differences: decrease step size or use higher-order formula w/ more points. Richardson 
          extrapolation is another option which uses two derivative estimates (ea with $O(h^2)$ error) to get a third
          (w/ $O(h^4)$ error), more accurate, approximation. \\
          \begin{math}
            h_2=\frac{h_1}{2};\hspace{10mm} D=\frac{4}{3}D(h_2)-\frac{1}{3}D(h_1)\\
          \end{math}\\
          \textbf{Ex:} using Rich's form, estimate first derivative @ x=0.5 w/ centered diff. approx. (e $O(h^2)$)
          w/ h=0.5 and h=0.25, all w/ previous ex.\\
          prev ex:
          \begin{math}
             f(x)=-0.1x^4-0.15x^3-0.25x+1.2\\
            D_{h=0.5}(x=0.5)=\frac{0.2-1.2}{1}=-1\\
            D_{h=0.25}(x=0.5)=\frac{0.6363-1.103}{0.5}=-0.9343\\
          \end{math}\\
            The improved estimate is:\\
          \begin{math}
            D=\frac{4}{3}(-0.9343)-\frac{1}{3}(-1) = -0.9125\\ 
            D_{h=0.25}(x=0.5)=\frac{0.6363-1.103}{0.5}=-0.9343\\
            ...\\
            D=\frac{4}{3}(-0.9343)-\frac{1}{3}(-1)=-0.9125\\
          \end{math}
          
          \item \textbf{Derivatives of Unequally Spaced Data}: Derivatives before required equally spaced data,
            moreso for the $O(h^2)$. Without that, we can use Lagrange interpolating polynomials, calculate the 
            first derivatives of them, and use that for your estimates. You only need three points for the 2nd-order
            lagrange formula and they do not need to be evenly spaced. Included in the next item is the explanation
            for Lagrange. Below is the derivative form of a second-order Lagrange interpolating polynomial.\\
            \begin{math}
              f'(x)=f(x_{i-1})\frac{2x-x_i-x_{i+1}}{(x_{i-1}-x_i)(x_{i-1}-x_{i+1})} +\\
              f(x_i)\frac{2x-x_{i-1}-x_{i+1}}{(x_i-x_{i-1})(x_i-x_{i+1})} + \\
              f(x_{i+1})\frac{2x-x_{i-1}-x_i}{(x_{i+1}-x_{i-1})(x_{i+1}-x_i)}
            \end{math}

          \item \textbf{Lagrange Interpolating Polynomials}: The Lagrange interpolating polynomial is simply a reformulation of the
                          Newton’s polynomial that avoids the computation of divided differences.\\
            \begin{math}
              f_n(x)=\sum_{n}{i=0}{\ell_i(x)f(x_i)} \\
              \ell_j(x)=\prod_{\begin{smallmatrix}0\le m\le k\\ m\neq j\end{smallmatrix}}\frac{x-x_m}{x_j-x_m}\\
              = \frac{(x-x_0)}{(x_j-x_0)} \cdots \frac{(x-x_{j-1})}{(x_j-x_{j-1})} \frac{(x-x_{j+1})}{(x_j-x_{j+1})} \cdots \frac{(x-x_k)}{(x_j-x_k)} \\
              \\
              f_1(x)=\frac{x-x_1}{x_0-x_1}f(x_0)+\frac{x-x_0}{x_1-x_0}f(x_1)\\
              f_2(x)=\frac{(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)}f(x_0) \\+\frac{(x-x_0)(x-x_2)}{(x_1-x_0)(x_1-x_2)}f(x_1)}+\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)}
            \end{math}
            
    \end{enumerate}
    
  \section{Formulas}
      \begin{itemize}
        \item \textbf{True Error}: $E_t =$ True value - Approximation ($\pm$)
        \item \textbf{True \% Rel. Error}: $\epsilon_t = (\frac{\text{True value} - \text{Approximation}}{\text{True Value}})\cdot100\%$
        \item \textbf{Aprox. Rel. Error}: $\epsilon_a =  |\frac{\text{This Approx.} - \text{Last Approx}}{\text{This Approx.}}|\cdot100\%$
      \end{itemize}
      
  \section{Algorithms}
    
      
  \section{Reference}
    \subsection{Matlab}
      \begin{itemize}
        \item \textbf{Common Functions}:
          sqrt, 
          exp, 
          abs,  
          log, 
          log10, 
          factorial, 
          sin, 
          sind, 
          cos,  
          cosd, 
          tan, 
          tand, 
          cot, 
          cotd, 
          round, 
          fix, 
          floor, 
          rem(x,y)\textit{mod}
          ceil, 
          sprintf
        \item \textbf{Matrices} are defined in many ways, here are the most common:
          \lstset{language=Matlab}
          \begin{lstlisting}
a = [ 1 2 3 4 5 ]
b = [ 1 2; 3 4; 5 6 ]
c = [ 1 2
      3 4
      5 6 ]
          \end{lstlisting}%
          $%
          a = \left( 1 2 3 4 5 \right)\hspace{1mm}
          b = \left( \begin{array}{cc}
                      1 & 2 \\
                      3 & 4 \\ 
                      5 & 6 \\
                    \end{array}
                    \right)
          c = b
          $
        \end{itemize}
    \subsection{Matlab Examples}
      \begin{itemize}
        \item Bisection function in Matlab. Passes f(x), lower x, upper x, and a error / upper i.
          \lstset{language=Matlab,basicstyle=\footnotesize,breaklines=true}
        \begin{lstlisting}
% A sample call:
%bisection2(@func1, -2, 4,
% 0.001, 500)
function root = bisection(fx, xl,
  xu, es, imax);

if fx(xl)*fx(xu) > 0 % if 
% guesses do not bracket
    disp('no bracket')
    return
end

for i=1:1:imax
   xr=(xu+xl)/2
   ea = abs((xu-xl)/xl);
   test= fx(xl)*fx(xr);
   if test < 0
       xu=xr;
   end
   if test > 0 xl=xr; end
   if test == 0 ea=0; end
   if ea < es break; end
end
%root = xr, it# = i, fx is original function
        \end{lstlisting}
        \item Newton's Method example code
          \lstset{language=Matlab,basicstyle=\footnotesize,breaklines=true}
          \begin{lstlisting}
function root = newtraph(func,dfunc,xr,es,maxit)
% Newton-Raphson method to find the root of a function
% call syntax:   newtraph(@func,@dfunc,xguess,es,maxit)
% 'func'(function), and 'dfunc'(its derivative) are defined 
% in files func.m and dfunc.m in the same directory
% inputs:
%   func = name of function 
%   dfunc = name of derivative of function 
%   xr = initial guess
%   es = stopping criterion (%)
%   maxit = (optional) maximum allowable iterations
% output:
%   root = real root

iter = 0;
while (1)
  xrold = xr;
  xr = xr - func(xr)/dfunc(xr);
  iter = iter + 1;
  if xr ~= 0, ea = abs((xr - xrold)/xr) * 100; end
  if ea <= es | iter >= maxit, break, end
end
fprintf('\n  Root= %f    #Iterations = %d \n', xr,iter);
fprintf(' How close is f(root) to zero?  f(root)= %f  \n', func(xr));
          \end{lstlisting}
        \end{itemize}
    \subsection{Math}
      \begin{itemize}
      \item \textbf{Taylor Series}: Provides a means to predict the value of a function at one point in terms of
             the function value and its derivatives at another. The function must be infinitely differentiable
             at f(a). An infinite number of terms yields an exact result for $x=a$.
        \begin{itemize}
          \item $f(x), x=a$: $\sum_{n=0} ^ {\infty} \frac {f^{(n)}(a)}{n!} \, (x-a)^{n} + R_n$
          \item Remainder, h is step size and $\epsilon$ is the value which makes the final
          equation exact for the nth derivative: \\ $R_n = \frac{f^{n+1}(\epsilon)}{(n+1)!} (x_{i+1}-x_i)^{n+1}h_{n+1}$ \\
          
        \end{itemize}
      \item \textbf{Matrix Multiplication}: Matrix multiplication is done by going right on the left matrix while going
        down on the right. After a row on the left is done, move to the next and repeat with the next column. Matrices'
        sizes are denoted by c x r, and for two matrices to multiply together, $r_1 = c_2$. Examples:
        \begin{itemize} %thanks wikipedia
          \item $\mathbf{A} = \begin{pmatrix} 
                a & b
                \end{pmatrix}\,, \quad \mathbf{B} = \begin{pmatrix} 
                x \\
                y 
                \end{pmatrix}\,,
                $
                $
                  \begin{pmatrix} 
                  a & b 
                  \end{pmatrix} \begin{pmatrix} 
                  x \\
                  y \\
                  \end{pmatrix} = ax + by \,,
                $ \\
                $
                  \begin{pmatrix} 
                  x \\
                  y \\
                  \end{pmatrix}\begin{pmatrix} 
                  a & b
                  \end{pmatrix} = \begin{pmatrix} 
                  xa & xb \\
                  ya & yb 
                  \end{pmatrix} \,.
                $

        \end{itemize}
      \end{itemize}
  \end{multicols}
\end{document}

